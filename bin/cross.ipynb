{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kjun1/CMVC/blob/main/cross.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5b4otDgBu4R"
   },
   "source": [
    "タスク\n",
    "\n",
    "とりあえず論文読み直し\n",
    "\n",
    "WaveRNN読む\n",
    "\n",
    "voice encoderの最終層に非時系列化層の追加 or broadcast versionの意味理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-mdUdwlYoU6M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dsN60CJnSzMz"
   },
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('../')\n",
    "from cmvc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo8MkhuMQP5_"
   },
   "source": [
    "# レイヤー作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OCPbW8hYisgb"
   },
   "outputs": [],
   "source": [
    "class CBGLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Conv+Bn+GLU\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride,padding=0):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                           out_channels=out_channels,\n",
    "                           kernel_size=kernel_size,\n",
    "                           stride = stride,\n",
    "                           padding=padding)\n",
    "    self.conv2 = nn.Conv2d(in_channels=in_channels,\n",
    "                           out_channels=out_channels,\n",
    "                           kernel_size=kernel_size,\n",
    "                           stride = stride,\n",
    "                           padding=padding)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x1 = self.bn1(self.conv1(x))\n",
    "    x2 = self.bn2(self.conv2(x))\n",
    "\n",
    "    x = torch.cat((x1,x2),1)\n",
    "    x = nn.functional.glu(x,1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CPekpuhB6Eh_"
   },
   "outputs": [],
   "source": [
    "class CBLLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Conv+Bn+LReLU\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride,padding=0):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride = stride,\n",
    "                          padding=padding)\n",
    "\n",
    "    self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.lrelu(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2DrpT_HajBwK"
   },
   "outputs": [],
   "source": [
    "class DBGLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Deconv + Bn + GLU\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride,padding=0):\n",
    "    super().__init__()\n",
    "    self.deconv1 = nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      stride=stride,\n",
    "                                      padding=padding)\n",
    "    self.deconv2 = nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      stride=stride,\n",
    "                                      padding=padding)\n",
    "    \n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x1 = self.bn1(self.deconv1(x))\n",
    "    x2 = self.bn2(self.deconv2(x))\n",
    "\n",
    "    x = torch.cat((x1,x2),1)\n",
    "    x = nn.functional.glu(x,1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OJEi7s7_jBhW"
   },
   "outputs": [],
   "source": [
    "class DBSLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Deconv + Bn + SoftPlus\n",
    "  \"\"\"\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "    super().__init__()\n",
    "    self.deconv = nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     stride=stride,\n",
    "                                     padding=padding)\n",
    "    \n",
    "    self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    self.softplus = nn.Softplus()\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.deconv(x)\n",
    "    x = self.bn(x)\n",
    "    x = self.softplus(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "az9dA6I2TG89"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  (N, C, H, W)を(N, C*H*W)にする\n",
    "  \"\"\"\n",
    "  def forward(self, x):\n",
    "    sizes = x.size()\n",
    "    return x.view(sizes[0],  -1)\n",
    "\n",
    "class ReshapeLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  (N, C*H*W)を(N, C, H, W)にする\n",
    "  \"\"\"\n",
    "  def forward(self, x, out_channel):\n",
    "    sizes = x.size()\n",
    "    h = int((sizes[1]/out_channel)**0.5)\n",
    "    return x.view(sizes[0],  out_channel, h, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6rigdOIFDBH"
   },
   "source": [
    "# Encoder Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qOwbSt6QUMP"
   },
   "source": [
    "## UttrEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THq7r9ZFJEbz"
   },
   "outputs": [],
   "source": [
    "class Hyper_UE:\n",
    "  x_c = 1\n",
    "  \n",
    "  d1_k = (3,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EmOhiR62P-25"
   },
   "outputs": [],
   "source": [
    "class UttrEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "    self.uttr_enc_d1 = CBGLayer(in_channels=1,\n",
    "                                out_channels=16,\n",
    "                                kernel_size=(3, 9),\n",
    "                                stride = (1,1),\n",
    "                                padding=(1,4))\n",
    "    \n",
    "    self.uttr_enc_d2 = CBGLayer(in_channels=16,\n",
    "                                out_channels=32,\n",
    "                                kernel_size=(4, 8),\n",
    "                                stride = (2,2),\n",
    "                                padding=(1,3))    \n",
    "\n",
    "    self.uttr_enc_d3 = CBGLayer(in_channels=32,\n",
    "                                out_channels=32,\n",
    "                                kernel_size=(4, 8),\n",
    "                                stride = (2,2),\n",
    "                                padding=(1,3))    \n",
    "    \n",
    "    self.uttr_enc_d4 = nn.Conv2d(in_channels=32,\n",
    "                             out_channels=16,\n",
    "                             kernel_size=(9, 5),\n",
    "                             stride = (9,1),\n",
    "                             padding=(0,2))\n",
    "\n",
    "  def uttr_encoder(self, x):\n",
    "    \"\"\"\n",
    "    音声のencoder\n",
    "    \"\"\"\n",
    "\n",
    "    x = self.uttr_enc_d1(x)\n",
    "    x = self.uttr_enc_d2(x)\n",
    "    x = self.uttr_enc_d3(x)\n",
    "    x = self.uttr_enc_d4(x)\n",
    "\n",
    "    mean, log_var = torch.split(x, 8, dim=1) # 半分\n",
    "     \n",
    "    return mean, log_var\n",
    "\n",
    "  def uttr_sample_z(self, mean, log_var):\n",
    "    \"\"\"\n",
    "    音声の潜在変数出すやつ\n",
    "    \"\"\"\n",
    "    epsilon = torch.randn(mean.shape).to(device)\n",
    "    return mean + torch.exp(log_var) * epsilon\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean, log_var = self.uttr_encoder(x)\n",
    "    z = self.uttr_sample_z(mean, log_var)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKZAdclFRRJa"
   },
   "source": [
    "## FaceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wGzn1LTMLVGJ"
   },
   "outputs": [],
   "source": [
    "class FaceEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "    self.face_enc_d1 = nn.Sequential(nn.Conv2d(in_channels=3,\n",
    "                                               out_channels=32,\n",
    "                                               kernel_size=(6, 6),\n",
    "                                               stride = (2,2),\n",
    "                                               padding=(2,2)),\n",
    "                                     nn.LeakyReLU())\n",
    "\n",
    "    self.face_enc_d2 = CBLLayer(in_channels=32,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=(6, 6),\n",
    "                                stride = (2,2),\n",
    "                                padding=(2,2))\n",
    "    \n",
    "    self.face_enc_d3 = CBLLayer(in_channels=64,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=(4, 4),\n",
    "                                stride = (2,2),\n",
    "                                padding=(1,1))\n",
    "    \n",
    "    self.face_enc_d4 = CBLLayer(in_channels=128,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=(4, 4),\n",
    "                                stride = (2,2),\n",
    "                                padding=(1,1)\n",
    "                                )\n",
    "    \n",
    "    self.face_enc_d5 = CBLLayer(in_channels=128,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=(4, 4),\n",
    "                                stride = (2,2),\n",
    "                                padding=(1,1))\n",
    "    \n",
    "    self.face_enc_d6 = FlattenLayer() #flattenまでに(n, c, 1, 1)になってる前提\n",
    "\n",
    "    self.face_enc_d7 = nn.Sequential(nn.Linear(256,256),\n",
    "                                     nn.LeakyReLU())\n",
    "    \n",
    "    self.face_enc_d8 = nn.Sequential(nn.Linear(256,16),\n",
    "                                     nn.LeakyReLU())\n",
    "\n",
    "  def face_encoder(self, y):\n",
    "    \"\"\"\n",
    "    顔面のencoder\n",
    "    \"\"\"\n",
    "\n",
    "    y = self.face_enc_d1(y)\n",
    "    y = self.face_enc_d2(y)\n",
    "    y = self.face_enc_d3(y)\n",
    "    y = self.face_enc_d4(y)\n",
    "    y = self.face_enc_d5(y)\n",
    "    y = self.face_enc_d6(y)\n",
    "    y = self.face_enc_d7(y)\n",
    "    y = self.face_enc_d8(y)\n",
    "\n",
    "    \n",
    "    \n",
    "    mean, log_var = torch.split(y, 8, dim=1) # 半分\n",
    "     \n",
    "    return mean, log_var\n",
    "\n",
    "  def face_sample_z(self, mean, log_var):\n",
    "    \"\"\"\n",
    "    顔面の潜在変数出すやつ\n",
    "    \"\"\"\n",
    "    epsilon = torch.randn(mean.shape).to(device)\n",
    "    return mean + torch.exp(log_var) * epsilon\n",
    "\n",
    "  def forward(self, y):\n",
    "    mean, log_var = self.face_encoder(y)\n",
    "    z = self.face_sample_z(mean, log_var)\n",
    "    z = z.unsqueeze(-1).unsqueeze(-1)\n",
    "    return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDpkUgk4fTPy"
   },
   "source": [
    "## VoiceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V3XiNPxqfd58"
   },
   "outputs": [],
   "source": [
    "class VoiceEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "    self.voice_enc_d1 = CBGLayer(in_channels=1,\n",
    "                                 out_channels=32,\n",
    "                                 kernel_size=(3,9),\n",
    "                                 stride=(1,1),\n",
    "                                 padding=(1,4))\n",
    "    \n",
    "    self.voice_enc_d2 = CBGLayer(in_channels=32,\n",
    "                                 out_channels=64,\n",
    "                                 kernel_size=(4,8),\n",
    "                                 stride=(2,2),\n",
    "                                 padding=(1,3))\n",
    "    \n",
    "    self.voice_enc_d3 = CBGLayer(in_channels=64,\n",
    "                                 out_channels=128,\n",
    "                                 kernel_size=(4,8),\n",
    "                                 stride=(2,2),\n",
    "                                 padding=(1,3))\n",
    "    \n",
    "    self.voice_enc_d4 = CBGLayer(in_channels=128,\n",
    "                                 out_channels=128,\n",
    "                                 kernel_size=(4,8),\n",
    "                                 stride=(2,2),\n",
    "                                 padding=(1,3))\n",
    "    \n",
    "    self.voice_enc_d5 = CBGLayer(in_channels=128,\n",
    "                                 out_channels=128,\n",
    "                                 kernel_size=(4,5),\n",
    "                                 stride=(4,1),\n",
    "                                 padding=(0,2))\n",
    "    \n",
    "    self.voice_enc_d6 = CBGLayer(in_channels=128,\n",
    "                                 out_channels=64,\n",
    "                                 kernel_size=(1,5),\n",
    "                                 stride=(1,1),\n",
    "                                 padding=(0,2))\n",
    "    \n",
    "    self.voice_enc_d7 = nn.Conv2d(in_channels=64,\n",
    "                                  out_channels=16,\n",
    "                                  kernel_size=(1,5),\n",
    "                                  stride=(1,1),\n",
    "                                 padding=(0,2))\n",
    "                                 \n",
    "\n",
    "  def voice_encoder(self, x):\n",
    "    \"\"\"\n",
    "    顔面持ってくるencoder\n",
    "    \"\"\"\n",
    "    x = self.voice_enc_d1(x)\n",
    "    x = self.voice_enc_d2(x)\n",
    "    x = self.voice_enc_d3(x)\n",
    "    x = self.voice_enc_d4(x)\n",
    "    x = self.voice_enc_d5(x)\n",
    "    x = self.voice_enc_d6(x)\n",
    "    x = self.voice_enc_d7(x)\n",
    "        \n",
    "    \"\"\"\n",
    "    第4層のConv2d出力のchannelの半分でmean半分でlog_varを予測している？\n",
    "    \"\"\"\n",
    "    mean, log_var = torch.split(x, 8, dim=1) # 半分\n",
    "     \n",
    "    return mean, log_var\n",
    "\n",
    "  def voice_sample_z(self, mean, log_var):\n",
    "    \"\"\"\n",
    "    音声の潜在変数出すやつ\n",
    "    \"\"\"\n",
    "    epsilon = torch.randn(mean.shape).to(device)\n",
    "    return mean + torch.exp(log_var) * epsilon\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean, log_var = self.voice_encoder(x)\n",
    "    z = self.voice_sample_z(mean, log_var)\n",
    "\n",
    "    z = z.squeeze(-1).squeeze(-1)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lyIg0rjo6bY"
   },
   "source": [
    "## UttrDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UXhR1Ihwo8HI"
   },
   "outputs": [],
   "source": [
    "class UttrDecoder(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.uttr_dec_d1 = DBGLayer(in_channels=8,\n",
    "                                out_channels=16,\n",
    "                                kernel_size=(9,5),\n",
    "                                stride=(9,1),\n",
    "                                padding=(0,2))\n",
    "  \n",
    "    self.uttr_dec_d2 = DBGLayer(in_channels=16,\n",
    "                                out_channels=16,\n",
    "                                kernel_size=(4,8),\n",
    "                                stride=(2,2),\n",
    "                                padding=(1,3))\n",
    "  \n",
    "    self.uttr_dec_d3 = DBGLayer(in_channels=16,\n",
    "                                out_channels=8,\n",
    "                                kernel_size=(4,8),\n",
    "                                stride=(2,2),\n",
    "                                padding=(1,3))\n",
    "  \n",
    "    self.uttr_dec_d4 = nn.ConvTranspose2d(in_channels=8,\n",
    "                                          out_channels=2,\n",
    "                                          kernel_size=(3,9),\n",
    "                                          stride=(1,1),\n",
    "                                          padding=(1,4))\n",
    "  \n",
    "  def uttr_decoder(self, z, c):\n",
    "    #print(z.size())\n",
    "    \n",
    "    x,_ = torch.broadcast_tensors(z, c)\n",
    "    x = self.uttr_dec_d1(x)\n",
    "    #print(x.size())\n",
    "    \n",
    "    x,_ = torch.broadcast_tensors(x, torch.cat((c, c),1))\n",
    "    x = self.uttr_dec_d2(x)\n",
    "    #print(x.size())\n",
    "\n",
    "    x,_ = torch.broadcast_tensors(x, torch.cat((c, c),1))\n",
    "    x = self.uttr_dec_d3(x)\n",
    "    #print(x.size())\n",
    "\n",
    "    x,_ = torch.broadcast_tensors(x, c)\n",
    "    x = self.uttr_dec_d4(x)\n",
    "    #print(x.size())\n",
    "\n",
    "    mean, log_var = torch.split(x, 1, dim=1) # 半分\n",
    "     \n",
    "    return mean, log_var\n",
    "  \n",
    "\n",
    "  def uttr_sample_z(self, mean, log_var):\n",
    "\n",
    "    epsilon = torch.randn(mean.shape).to(device)\n",
    "    return mean + torch.exp(log_var) * epsilon\n",
    "  \n",
    "\n",
    "  def forward(self, z, c):\n",
    "    mean, log_var = self.uttr_decoder(z, c)\n",
    "    z = self.uttr_sample_z(mean, log_var)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxYKns4o8O6"
   },
   "source": [
    "## FaceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NSb0v98zo8WQ"
   },
   "outputs": [],
   "source": [
    "class FaceDecoder(nn.Module):\n",
    "  def __init__(self):\n",
    "\n",
    "    super().__init__()\n",
    "    self.face_dec_d1 = nn.Sequential(nn.Linear(8,128),\n",
    "                                     nn.Softplus())\n",
    "    \n",
    "    self.face_dec_d2 = nn.Sequential(nn.Linear(128,2048),\n",
    "                                     nn.Softplus())\n",
    "    \n",
    "    self.face_dec_d3 = ReshapeLayer()\n",
    "\n",
    "    self.face_dec_d4 = DBSLayer(in_channels=128,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=(3,3),\n",
    "                                stride=(2,2),\n",
    "                                padding=(2,2))\n",
    "\n",
    "    self.face_dec_d5 = DBSLayer(in_channels=128,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=(6,6),\n",
    "                                stride=(2,2),\n",
    "                                padding=(2,2))\n",
    "    \n",
    "    self.face_dec_d6 = DBSLayer(in_channels=128,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=(6,6),\n",
    "                                stride=(2,2),\n",
    "                                padding=(2,2))\n",
    "    \n",
    "    self.face_dec_d7 = DBSLayer(in_channels=64,\n",
    "                                out_channels=32,\n",
    "                                kernel_size=(6,6),\n",
    "                                stride=(2,2),\n",
    "                                padding=(2,2))\n",
    "    \n",
    "    self.face_dec_d8 = nn.Conv2d(in_channels=32,\n",
    "                                 out_channels=6,\n",
    "                                 kernel_size=(5,5),\n",
    "                                 stride=(1,1))\n",
    "    \n",
    "    \n",
    "\n",
    "  def face_decoder(self, c):\n",
    "\n",
    "    y = self.face_dec_d1(c)\n",
    "    y = self.face_dec_d2(y)\n",
    "    y = self.face_dec_d3(y, 128)\n",
    "    y = self.face_dec_d4(y)\n",
    "    y = self.face_dec_d5(y)\n",
    "    y = self.face_dec_d6(y)\n",
    "    y = self.face_dec_d7(y)\n",
    "    y = self.face_dec_d8(y)\n",
    "\n",
    "    mean, log_var = torch.split(y, 3, dim=1) # 半分\n",
    "     \n",
    "    return mean, log_var\n",
    "\n",
    "  def face_sample_z(self, mean, log_var):\n",
    "    \"\"\"\n",
    "    顔面の潜在変数出すやつ\n",
    "    \"\"\"\n",
    "    epsilon = torch.randn(mean.shape).to(device)\n",
    "    return mean + torch.exp(log_var) * epsilon\n",
    "\n",
    "  def forward(self, y):\n",
    "    mean, log_var = self.face_decoder(y)\n",
    "    z = self.face_sample_z(mean, log_var)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdXpdJmqRWSJ"
   },
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cqiYSptfIIYU"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \n",
    "    \n",
    "    super().__init__()\n",
    "    self.ue = UttrEncoder()\n",
    "    self.ud = UttrDecoder()\n",
    "    self.fe = FaceEncoder()\n",
    "    self.fd = FaceDecoder()\n",
    "    self.ve = VoiceEncoder()\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    z = self.ue(x)\n",
    "    c = self.fe(y)\n",
    "    x_hat = self.ud(z, c)\n",
    "    print(x_hat.size())\n",
    "    c_hat = self.ve(x_hat)\n",
    "    print(c_hat.size())\n",
    "    y_hat = self.fd(c_hat)\n",
    "\n",
    "    return y_hat\n",
    "  \n",
    "  def loss(self, x, y):\n",
    "    \"\"\"\n",
    "    reconstruction + KL divergence\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARvrKuOqRYEQ"
   },
   "source": [
    "# 確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgAyE07yFZHN"
   },
   "source": [
    "## net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "90Ulr38JFaoB"
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0dbSrdMDb9W"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZDuUNLWDhJd",
    "outputId": "a0eca7db-0936-4a0e-df42-f61824294bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 36, 8])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 3, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((2,1,36,8))\n",
    "y = torch.ones((2,3,32,32))\n",
    "\n",
    "net.train()\n",
    "\n",
    "x = x.to(device) \n",
    "y = y.to(device)\n",
    "print(net.forward(x, y).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzwyizxEDapy"
   },
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQBwklgN2nV-",
    "outputId": "c8cc9cc9-9b89-449e-952f-661cca92edca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 36, 8])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 3, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1,1,36,8))\n",
    "y = torch.ones((1,3,32,32))\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "  x = x.to(device) \n",
    "  y = y.to(device)\n",
    "  print(net.forward(x, y).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH4mE_emuM6R"
   },
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mu8XxrZ7Rm5v"
   },
   "outputs": [],
   "source": [
    "model = FaceEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppEyA8jebuai",
    "outputId": "b16482ea-75a7-480d-e7cd-ead6e064277b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1,3,32,32))\n",
    "\"\"\"\n",
    "入力\n",
    "1: バッチサイズ\n",
    "2: channel\n",
    "3: mfcc_size(画像における縦)\n",
    "4: uttr_len(画像における横)\n",
    "\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  x = x.to(device) \n",
    "  print(model.forward(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGoWuQa4uPc4"
   },
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUoTbkMUuTa1"
   },
   "outputs": [],
   "source": [
    "model = UttrDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clKTsu3HuUZJ",
    "outputId": "d598ce8c-910b-45e7-bf1d-40b79bb3e0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 36, 400])\n"
     ]
    }
   ],
   "source": [
    "z = torch.ones((1,8,1,100))\n",
    "c = torch.ones((1,8,1,1))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  c = c.to(device) \n",
    "  print(model.forward(z, c).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iitJL9InAUrk"
   },
   "source": [
    "## broadcastの仕組み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9jgOIrBAZXF",
    "outputId": "631cc7a6-69e7-4de8-8951-b2f36d7a8ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
      "torch.Size([1, 8, 9, 100])\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]])\n",
      "torch.Size([1, 8, 1, 1])\n",
      "tensor([[[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1,8,9,100))\n",
    "print(x)\n",
    "print(x.size())\n",
    "y = torch.ones((1,8,1,1))\n",
    "print(y)\n",
    "print(y.size())\n",
    "\n",
    "print(x+y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDEdXwkFVgwqg3rLVB2dnu",
   "collapsed_sections": [
    "yo8MkhuMQP5_",
    "8qOwbSt6QUMP",
    "kKZAdclFRRJa",
    "BDpkUgk4fTPy",
    "rfxYKns4o8O6",
    "sdXpdJmqRWSJ",
    "HH4mE_emuM6R",
    "iitJL9InAUrk"
   ],
   "include_colab_link": true,
   "name": "cross.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
